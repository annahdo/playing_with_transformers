{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4177ac",
   "metadata": {},
   "source": [
    "# Activation addition in Llama with custom wrappers\n",
    "\n",
    "This notebook shows how to extract and manipulate internal activations of Llama Transformer model. All you need is access to a trained model (either you have it downloaded locally and update the `model_path` accordingly or you have access to models via Huggingface and get an [authentication token](https://huggingface.co/docs/hub/security-tokens).) You also might wanna make sure you have a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d78bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3229f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3965aad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7c61d243394cfd87239f32f1005fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "# model_path = \"/data/private_models/cais_models/llama/llama_hf_weights_v1.1/llama-7b\"\n",
    "model_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=auth_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, use_auth_token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac190963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee6d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping classes\n",
    "\n",
    "class WrappedBlock(torch.nn.Module):\n",
    "    def __init__(self, block, block_name):\n",
    "        super().__init__()\n",
    "        self.block_name = block_name\n",
    "        self.block = block\n",
    "        self.output = None\n",
    "        self.to_add = None\n",
    "        self.token_pos = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            self.output = output[0]\n",
    "            modified = output[0]\n",
    "        else:\n",
    "            self.output = output\n",
    "            modified = output\n",
    "            \n",
    "        if self.to_add is not None:\n",
    "            if len(self.to_add.shape)>1:\n",
    "                # add the several individual streams of activation vec to beginning\n",
    "                assert self.to_add.shape[0] <= modified.shape[1], \"to_add residuals longer than output\"\n",
    "                modified[:, :self.to_add.shape[0], :] = modified[:, :self.to_add.shape[0], :] + self.to_add.unsqueeze(0)                \n",
    "            elif self.token_pos:\n",
    "                assert len(self.to_add.shape) == 1, \"to_add should be only one vector\"\n",
    "                print(self.token_pos)\n",
    "                for pos in self.token_pos:\n",
    "                    modified[:, pos, :] = modified[:, pos, :] + self.to_add.unsqueeze(0)  \n",
    "            else: \n",
    "                assert len(self.to_add.shape) == 1, \"to_add should be only one vector\"\n",
    "                modified = modified + self.to_add \n",
    "            \n",
    "        if isinstance(output, tuple):\n",
    "            output = (modified,) + output[1:] \n",
    "        else:\n",
    "            output = modified\n",
    "        return output\n",
    "\n",
    "    def set_to_add(self, activations, token_pos):\n",
    "        self.to_add = activations.squeeze()\n",
    "        self.token_pos = token_pos\n",
    "        \n",
    "    def reset(self):\n",
    "        self.output = None\n",
    "        self.to_add = None\n",
    "        self.token_pos = None\n",
    "\n",
    "    \n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, layer_id, block_name):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.layer_id = layer_id\n",
    "        self.block_name = block_name\n",
    "        \n",
    "        # make sure model is not wrapped twice\n",
    "        self.model = self._unwrap()\n",
    "        self.wrapped_block = None\n",
    "\n",
    "        self._wrap_block()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=100, random_seed=0):\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_new_tokens=max_new_tokens, use_cache=False)\n",
    "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    def generate_with_reset(self, prompt, max_new_tokens=100, random_seed=0):\n",
    "        output = self.run_prompt(prompt)\n",
    "        self.reset_all()\n",
    "        generate_ids = self.model.generate(past_key_val=output.past_key_values, \n",
    "                                           use_cache=True,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    \n",
    "    def get_logits(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tokens.to(self.device)).logits\n",
    "            return logits\n",
    "        \n",
    "    def run_prompt(self, prompt, **kwargs):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\") \n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        output = self.model(input_ids, **kwargs)\n",
    "        return output\n",
    " \n",
    "    def _wrap_block(self):\n",
    "        if self.block_name == 'self_attn':\n",
    "            block = self.model.model.layers[self.layer_id].self_attn\n",
    "            self.model.model.layers[self.layer_id].self_attn = WrappedBlock(block,\"self_attn\")\n",
    "            self.wrapped_block = self.model.model.layers[self.layer_id].self_attn\n",
    "        elif self.block_name == 'mlp':\n",
    "            block = self.model.model.layers[self.layer_id].mlp\n",
    "            self.model.model.layers[layer_id].mlp = WrappedBlock(block,\"mlp\")\n",
    "            self.wrapped_block = self.model.model.layers[self.layer_id].mlp\n",
    "        elif self.block_name == 'input_layernorm':\n",
    "            block = self.model.model.layers[self.layer_id].input_layernorm\n",
    "            self.model.model.layers[layer_id].input_layernorm = WrappedBlock(block,\"input_layernorm\")\n",
    "            self.wrapped_block = self.model.model.layers[self.layer_id].input_layernorm\n",
    "        elif self.block_name == 'post_attention_layernorm':\n",
    "            block = self.model.model.layers[self.layer_id].post_attention_layernorm\n",
    "            self.model.model.layers[layer_id].post_attention_layernorm = WrappedBlock(block,\"post_attention_layernorm\")\n",
    "            self.wrapped_block = self.model.model.layers[self.layer_id].post_attention_layernorm\n",
    "        elif self.block_name == 'decoder_block':\n",
    "            block = self.model.model.layers[self.layer_id]\n",
    "            self.model.model.layers[self.layer_id] = WrappedBlock(block,\"decoder_block\")\n",
    "            self.wrapped_block = self.model.model.layers[self.layer_id]\n",
    "        else:\n",
    "            return f\"No wrapped block named {block_name}.\"\n",
    "\n",
    "            \n",
    "    def get_activations(self):\n",
    "        return self.wrapped_block.output\n",
    "\n",
    "    def set_to_add(self, activations, token_pos=None):\n",
    "        self.wrapped_block.set_to_add(activations, token_pos)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.wrapped_block.reset()\n",
    "    \n",
    "    def _is_wrapped(self, block):\n",
    "        if hasattr(block, 'block'):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _unwrap(self):\n",
    "        for l, layer in enumerate(self.model.model.layers):\n",
    "            if self._is_wrapped(layer):\n",
    "                self.model.model.layers[l] = layer.block\n",
    "            if self._is_wrapped(self.model.model.layers[l].self_attn):\n",
    "                    self.model.model.layers[l].self_attn = self.model.model.layers[l].self_attn.block\n",
    "            if self._is_wrapped(self.model.model.layers[l].mlp):\n",
    "                    self.model.model.layers[l].mlp = self.model.model.layers[l].mlp.block\n",
    "            if self._is_wrapped(self.model.model.layers[l].input_layernorm):\n",
    "                    self.model.model.layers[l].input_layernorm = self.model.model.layers[l].input_layernorm.block\n",
    "            if self._is_wrapped(self.model.model.layers[l].post_attention_layernorm):\n",
    "                    self.model.model.layers[l].post_attention_layernorm = self.model.model.layers[l].post_attention_layernorm.block\n",
    "\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba7fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def apply_activation_difference(wrapped_model, layer_id, block_name, sentences, prompt1, prompt2, coeff=150, token_pos=None, max_new_tokens=20, textfile=None):\n",
    "    \"\"\"\n",
    "    wrapped_model: our wrapped model\n",
    "    sentences: list of strings that are modified with internal activations\n",
    "    prompt1: first string to encode\n",
    "    prompt2: 2nd string to encode\n",
    "    coeff: coefficient with which to multiply normalized difference between prompt1 and prompt2\n",
    "    token_pos: list of token positions where the difference is applied (if set to None it is applied to every token)\n",
    "    block_name, layer_id, just for writing it in the text file\n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_string = f\"Parameters:\\n\\\n",
    "    prompt1: '{prompt1}'\\n\\\n",
    "    prompt2: '{prompt2}'\\n\\\n",
    "    coeff: {coeff}\\n\\\n",
    "    layer_id: {layer_id}\\n\\\n",
    "    block_name: '{block_name}'\\n\\\n",
    "    token_pos: {token_pos}\\n\\\n",
    "    max_new_tokens: {max_new_tokens}\\n\\n\\\n",
    "    \"\n",
    "\n",
    "\n",
    "    if textfile:\n",
    "        with open(textfile, 'w') as f:\n",
    "            f.write(formatted_string)\n",
    "            \n",
    "    def append_to_file(text):\n",
    "        if textfile:\n",
    "            with open(textfile, 'a') as f:\n",
    "                f.write(text)\n",
    "                \n",
    "    print(\"-\"*30)\n",
    "    print(\"-\"*30)\n",
    "    wrapped_model.reset()\n",
    "    print(\"No activation addition:\\n\")\n",
    "    append_to_file(\"\\nNo activation addition:\\n\\n\")\n",
    "    for sentence in sentences:\n",
    "        generated = wrapped_model.generate(sentence, max_new_tokens=max_new_tokens)\n",
    "        print(f\"{generated}\\n\")\n",
    "        append_to_file(f\"{generated}\\n\\n\")\n",
    "\n",
    "    # get internal activations\n",
    "    wrapped_model.reset()\n",
    "    wrapped_model.run_prompt(prompt1)\n",
    "    activations1 = wrapped_model.get_activations()\n",
    "    wrapped_model.reset()\n",
    "    wrapped_model.run_prompt(prompt2)\n",
    "    activations2 = wrapped_model.get_activations()\n",
    "\n",
    "    diff = activations1[0,-1,:]-activations2[0,-1,:]\n",
    "    diff = diff/diff.norm()\n",
    "\n",
    "    wrapped_model.reset()\n",
    "    wrapped_model.set_to_add(coeff*diff, token_pos=token_pos)\n",
    "    print(\"-\"*30 + \"\\nPositive coefficient:\\n\")\n",
    "    append_to_file(\"-\"*30 + \"\\nPositive coefficient:\\n\\n\")\n",
    "    for sentence in sentences:\n",
    "        generated = wrapped_model.generate(sentence, max_new_tokens=max_new_tokens)\n",
    "        print(f\"{generated}\\n\")\n",
    "        append_to_file(f\"{generated}\\n\\n\")\n",
    "        \n",
    "    wrapped_model.reset()\n",
    "    wrapped_model.set_to_add(-coeff*diff, token_pos=token_pos)\n",
    "    print(\"-\"*30 + \"\\nNegative coefficient:\\n\")\n",
    "    append_to_file(\"-\"*30 + \"\\nNegative coefficient:\\n\\n\")\n",
    "    for sentence in sentences:\n",
    "        generated = wrapped_model.generate(sentence, max_new_tokens=max_new_tokens)\n",
    "        print(f\"{generated}\\n\")\n",
    "        append_to_file(f\"{generated}\\n\\n\")\n",
    "\n",
    "        \n",
    "def make_new_file():# to save results\n",
    "    path = \"results/activation_addition\"\n",
    "    # Create directories recursively\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # Get current date and time\n",
    "    now = datetime.now()\n",
    "    # Format as string in the desired format (here as YearMonthDay_HourMinuteSecond)\n",
    "    filename = now.strftime(\"%Y%m%d_%H%M%S.txt\")\n",
    "    full_path = os.path.join(path, filename)\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ede3e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# small observation\n",
    "layer_id = 25\n",
    "wrapped_model = WrappedModel(model, tokenizer, layer_id=layer_id, block_name=\"decoder_block\")\n",
    "output = wrapped_model.run_prompt(\"some test prompt\", output_hidden_states=True, output_attentions=True, return_dict=True)\n",
    "# output.attentions are just the attention values NOT the self attention outputs\n",
    "# output.attentions[0].shape is torch.Size([1, 32, 3, 3])\n",
    "# len(output.attentions) is 32\n",
    "# output.hidden_states are the outputs of decoder_block, but shifted\n",
    "# output.hidden_states[0] are the inputs_embeds\n",
    "print((output.hidden_states[layer_id+1]==wrapped_model.model.model.layers[layer_id].output).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714816fa",
   "metadata": {},
   "source": [
    "# Activation addition\n",
    "\n",
    "Modify the code as you want. Important parameters are:\n",
    "\n",
    "* `layer_id`: layer at which activation addition is performed\n",
    "* `block_name`: block at which activation addition is performed. Must be one of `[\"decoder_block\", \"self_attn\", \"mlp\", 'input_layernorm\", \"post_attention_layernorm\"]`\n",
    "* `sentences`: list of strings that are modified with internal activations\n",
    "* `prompt1`: first string to encode\n",
    "* `prompt2`: 2nd string to encode\n",
    "* `coeff`: coefficient with which to multiply normalized difference between `prompt1` and `prompt2`\n",
    "* `token_pos`: list of token positions where the difference is applied (if set to None it is applied to every token)\n",
    "* `max_new_tokens`: how many new tokens to generate\n",
    "* `textfile`:       a textfile to save results to (you can generate a timestamped one with `make_new_file()`)\n",
    "\n",
    "The function `apply_activation_difference` does the activation addition by calculating internal representations of `prompt1` and `prompt2` (at the layer and block determined when wrapping the model). The difference is calculated and normalized. This difference is the activation that is added. The function then loops through the given `sentences` and adds the normalized difference multiplied by `coeff` to the internal activations of each sentence at `token_pos`, generating new text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35716837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP MODEL\n",
    "# layer_id:   layer at which activation addition is performed\n",
    "# block_name: block at which activation addition is performed. \n",
    "#             must be one of [\"decoder_block\", \"self_attn\", \"mlp\", 'input_layernorm\", \"post_attention_layernorm\"]\n",
    "layer_id=25\n",
    "block_name=\"decoder_block\"\n",
    "wrapped_model = WrappedModel(model, tokenizer, layer_id=layer_id, block_name=block_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6984095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences:      list of strings that are modified with internal activations\n",
    "# prompt1:        first string to encode\n",
    "# prompt2:        2nd string to encode\n",
    "# coeff:          coefficient with which to multiply normalized difference between prompt1 and prompt2\n",
    "# token_pos:      list of token positions where the difference is applied \n",
    "#                 (if set to None it is applied to every token)\n",
    "# max_new_tokens: how many new tokens to generate\n",
    "# textfile:       a textfile to save results to (you can generate a timestamped one with make_new_file())\n",
    "\n",
    "sentences = [\n",
    "    \"I think dogs are \", \n",
    "    \"I think cats are \", \n",
    "    \"I hate you because\",\n",
    "    \"I love you because\",\n",
    "    \"I think of you as\",    \n",
    "    \"I think you're\",\n",
    "    \"Today I feel\",\n",
    "]\n",
    "prompt1 = \"Love\"\n",
    "prompt2 = \"Hate\"\n",
    "coeff = 80\n",
    "token_pos = None\n",
    "max_new_tokens = 20\n",
    "textfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91dd40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "------------------------------\n",
      "No activation addition:\n",
      "\n",
      "I think dogs are 100% better than cats.\n",
      "I think dogs are 100% better\n",
      "\n",
      "I think cats are 100% better than dogs.\n",
      "I think cats are 100% better\n",
      "\n",
      "I hate you because you are beautiful.\n",
      "I hate you because you are beautiful. I hate you because you are beautiful\n",
      "\n",
      "I love you because you are you. I love you because you are you. I love you because you are you.\n",
      "\n",
      "I think of you as a friend.\n",
      "I think of you as a friend.\n",
      "I think of you as a friend\n",
      "\n",
      "I think you're right. I'm not sure what the problem is, but I'm sure it's\n",
      "\n",
      "Today I feel like I'm in a bit of a rut. I'm not sure what it is\n",
      "\n",
      "------------------------------\n",
      "Positive coefficient:\n",
      "\n",
      "I think dogs are 100% better than cats. I're a cat person, but I're\n",
      "\n",
      "I think cats are 100% better than dogs. I're a cat person.\n",
      "I're a\n",
      "\n",
      "I hate you because you are a woman. I hate you because you are a woman. I hate you because you are\n",
      "\n",
      "I love you because you are my best friend. I love you because you are my best friend. I love you because\n",
      "\n",
      "I think of you as a friend,\n",
      "I think of you as a friend,\n",
      "I think of you as a friend\n",
      "\n",
      "I think you're right, and I'm sure you're right. I'm just not sure how to\n",
      "\n",
      "Today I feel like I're in a dream\n",
      "I're in a dream\n",
      "I're in a\n",
      "\n",
      "------------------------------\n",
      "Negative coefficient:\n",
      "\n",
      "I think dogs are 100% better than cats.\n",
      "I think dogs are 100% better\n",
      "\n",
      "I think cats are 100% evil.\n",
      "I think they are evil because they are evil.\n",
      "I think\n",
      "\n",
      "I hate you because you are a fucking idiot.\n",
      "I hate you because you are a fucking\n",
      "\n",
      "I love you because you are you.\n",
      "I love you because you are you. I love you because you are you\n",
      "\n",
      "I think of you as a friend.\n",
      "I think of you as a friend.\n",
      "I think of you as a friend\n",
      "\n",
      "I think you're right. I think it's a good idea to have a separate group for the \"new\"\n",
      "\n",
      "Today I feel like a little girl again.\n",
      "I feel like Im 10 years old and I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apply_activation_difference(wrapped_model, layer_id, block_name, sentences, prompt1, prompt2, coeff=coeff, \n",
    "                            token_pos=token_pos, max_new_tokens=max_new_tokens, textfile=textfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbeb4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
