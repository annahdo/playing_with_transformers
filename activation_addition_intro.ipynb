{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8812879",
   "metadata": {},
   "source": [
    "# Activation addition in Llama with custom wrappers\n",
    "\n",
    "This notebook shows how to extract and manipulate internal activations of Llama Transformer model. All you need is access to a trained model (either you have it downloaded locally and update the `model_path` accordingly or you have access to models via Huggingface and get an [authentication token](https://huggingface.co/docs/hub/security-tokens).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d78bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ff3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"playing_with_transformers\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1043a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "import wrapping\n",
    "\n",
    "importlib.reload(wrapping)\n",
    "\n",
    "from wrapping import WrappedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3965aad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0482f0a7819c4cccaee49f4b4bb68162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ann_kathrin_dombrowski/miniconda3/envs/jup/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c109f76eb604929a5e07044ed6aab8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2045e895b44b178018faf2b30fc422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfd661a9369410f9e0fd235f7e429c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da0bece6b6473b960fcd731239886a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model_name = \"llama-7b\"\n",
    "model_path = f\"huggyllama/{model_name}\"\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=auth_token).to(device)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=auth_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' \n",
    "\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae31eb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "    (act_fn): SiLUActivation()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm()\n",
       "  (post_attention_layernorm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_id = 5\n",
    "# model before wrapping\n",
    "model.model.layers[layer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94261206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class variable output does not exist\n",
    "hasattr(model.model.layers[layer_id], \"output\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5b8fb",
   "metadata": {},
   "source": [
    "### Wrapping\n",
    "\n",
    "We want access to internal activations. By wrapping the model you make sure to have access to the internal layer activations. The wrapper class basically enables you to add this functionality to any layer and block in the model. If you want to wrap a different block, you can always call `wrapped_model.unwrap()` which unwraps the complete model. Calling `WrappedModel(model, tokenizer)` itself does not wrap any block/layer yet. This happens only when you call `wrapped_model.wrap_block(layer_id, block_name=block_name)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3bc2b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 5\n",
    "block_name = \"decoder_block\"\n",
    "\n",
    "# WRAP MODEL\n",
    "# create wrapped model\n",
    "wrapped_model = WrappedModel(model, tokenizer)\n",
    "# make sure nothing is wrapped from previous runs\n",
    "wrapped_model.unwrap()\n",
    "# wrap the block you want to wrap\n",
    "wrapped_model.wrap_block(layer_id, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "609bf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedBlock(\n",
       "  (block): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      (act_fn): SiLUActivation()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model after wrapping\n",
    "wrapped_model.model.model.layers[layer_id] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c77b4610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class variable output is empty as we have not run the model yet\n",
    "hasattr(model.model.layers[layer_id], \"output\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e04d33",
   "metadata": {},
   "source": [
    "### Getting the internal representations\n",
    "\n",
    "By wrapping the model you make sure to have access to the internal layer activations. If you now run the model, the output of your selected `layer_id` and `block_name` will be saved. This happens in the class `WrappedBlock` which you can find under modules/wrapping.py.\n",
    "It basically saves the output of your selected `layer_id` and `block_name` in the class variable `WrappedBlock.output`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dcd429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Love\"\n",
    "prompt2 = \"Hate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abe5660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded love_vec: torch.Size([1, 2, 4096])\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.run_prompt(prompt1)\n",
    "love_vec = wrapped_model.get_activations(layer_id, block_name=block_name)\n",
    "print(f\"shape of encoded love_vec: {love_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5181e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded hate_vec: torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.run_prompt(prompt2)\n",
    "hate_vec = wrapped_model.get_activations(layer_id, block_name=block_name)\n",
    "print(f\"shape of encoded hate_vec: {hate_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892d0ff",
   "metadata": {},
   "source": [
    "### Determining a direction\n",
    "\n",
    "The encoded representations might have different dimension. For example encoding \"Love\" basically gives you two tokens, the start of sentence token and the love token. \"Hate\" gives you 3 tokens, the start of sentence token and two tokens that correspond to \"Hate\".\n",
    "\n",
    "There are several ways how to now determine the difference between the two vectors. \n",
    "Lets just keep it simple and take the representation of the last token for both prompts and take the difference here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0472486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of love_direction: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "love_direction = love_vec[0, -1] - hate_vec[0, -1]\n",
    "\n",
    "# normalize\n",
    "love_direction /= love_direction.norm()\n",
    "\n",
    "hate_direction = -love_direction\n",
    "print(f\"shape of love_direction: {love_direction.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a4200",
   "metadata": {},
   "source": [
    "### Generating new text with activation addition\n",
    "\n",
    "Lets apply our direction to some sentence and see how it changes the generated text. We need to tell our model which direction to add. And then call the generate function. The current implementation (`token_pos=None`) adds the direction to all tokens of the input and also to all newly generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4bf33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the love direction to the vector that should be added\n",
    "coeff = 10\n",
    "wrapped_model.set_to_add(layer_id, coeff*love_direction, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9dd936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating love <3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are 100% pure and unconditional love.\n",
      "I think dogs are 100\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 20\n",
    "sentence = \"I think dogs are \"\n",
    "\n",
    "print(\"Generating love <3\\n\")\n",
    "generated = wrapped_model.generate(sentence, max_new_tokens=max_new_tokens)\n",
    "print(generated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c4eabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hate :(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are 100% evil.\n",
      "I think they are the most evil thing on the planet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrapped_model.set_to_add(layer_id, coeff*hate_direction, block_name=block_name)\n",
    "print(\"Generating hate :(\\n\")\n",
    "generated = wrapped_model.generate(sentence, max_new_tokens=max_new_tokens)\n",
    "print(generated[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
