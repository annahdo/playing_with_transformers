{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8812879",
   "metadata": {},
   "source": [
    "# Activation addition in Llama with baukit library\n",
    "\n",
    "This notebook shows how to extract and manipulate internal activations of a Llama Transformer model using the [baukit library](https://github.com/davidbau/baukit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d78bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "from baukit import Trace, module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3965aad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5bf9cbbb6741899198fd13f0a2d5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713be1188b344c95921db0e6f00284fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237c68ab39e94f0a8cd0c592fdff7206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91130e9b949e44cf916dc66f42821f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0c17b0a76e4ce6bcc5c537b4515df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5d37b60922458e8f1363ba38d85f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25865a22c8f749d1a1cc5b8494806deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9105d4e70d3e48a8a52df24a65b4792b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21be57c2459648df9a3d567c4794a49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96245b8dfd2c4a91aea8ce7d05b777c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6a1aa8aef74be48cfc14f1426b1af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model_name = \"llama-7b\"\n",
    "model_path = f\"huggyllama/{model_name}\"\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' \n",
    "\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc2df9",
   "metadata": {},
   "source": [
    "We first need to figure out the name of the layer where we want to do our activation addition. Lets focus on the residual stream output of layer 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee12f54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'model',\n",
       " 'model.embed_tokens',\n",
       " 'model.layers',\n",
       " 'model.layers.0',\n",
       " 'model.layers.0.self_attn',\n",
       " 'model.layers.0.self_attn.q_proj',\n",
       " 'model.layers.0.self_attn.k_proj',\n",
       " 'model.layers.0.self_attn.v_proj',\n",
       " 'model.layers.0.self_attn.o_proj',\n",
       " 'model.layers.0.self_attn.rotary_emb',\n",
       " 'model.layers.0.mlp',\n",
       " 'model.layers.0.mlp.gate_proj',\n",
       " 'model.layers.0.mlp.up_proj',\n",
       " 'model.layers.0.mlp.down_proj',\n",
       " 'model.layers.0.mlp.act_fn',\n",
       " 'model.layers.0.input_layernorm',\n",
       " 'model.layers.0.post_attention_layernorm',\n",
       " 'model.layers.1',\n",
       " 'model.layers.1.self_attn',\n",
       " 'model.layers.1.self_attn.q_proj',\n",
       " 'model.layers.1.self_attn.k_proj',\n",
       " 'model.layers.1.self_attn.v_proj',\n",
       " 'model.layers.1.self_attn.o_proj',\n",
       " 'model.layers.1.self_attn.rotary_emb',\n",
       " 'model.layers.1.mlp',\n",
       " 'model.layers.1.mlp.gate_proj',\n",
       " 'model.layers.1.mlp.up_proj',\n",
       " 'model.layers.1.mlp.down_proj',\n",
       " 'model.layers.1.mlp.act_fn',\n",
       " 'model.layers.1.input_layernorm',\n",
       " 'model.layers.1.post_attention_layernorm',\n",
       " 'model.layers.2',\n",
       " 'model.layers.2.self_attn',\n",
       " 'model.layers.2.self_attn.q_proj',\n",
       " 'model.layers.2.self_attn.k_proj',\n",
       " 'model.layers.2.self_attn.v_proj',\n",
       " 'model.layers.2.self_attn.o_proj',\n",
       " 'model.layers.2.self_attn.rotary_emb',\n",
       " 'model.layers.2.mlp',\n",
       " 'model.layers.2.mlp.gate_proj',\n",
       " 'model.layers.2.mlp.up_proj',\n",
       " 'model.layers.2.mlp.down_proj',\n",
       " 'model.layers.2.mlp.act_fn',\n",
       " 'model.layers.2.input_layernorm',\n",
       " 'model.layers.2.post_attention_layernorm',\n",
       " 'model.layers.3',\n",
       " 'model.layers.3.self_attn',\n",
       " 'model.layers.3.self_attn.q_proj',\n",
       " 'model.layers.3.self_attn.k_proj',\n",
       " 'model.layers.3.self_attn.v_proj',\n",
       " 'model.layers.3.self_attn.o_proj',\n",
       " 'model.layers.3.self_attn.rotary_emb',\n",
       " 'model.layers.3.mlp',\n",
       " 'model.layers.3.mlp.gate_proj',\n",
       " 'model.layers.3.mlp.up_proj',\n",
       " 'model.layers.3.mlp.down_proj',\n",
       " 'model.layers.3.mlp.act_fn',\n",
       " 'model.layers.3.input_layernorm',\n",
       " 'model.layers.3.post_attention_layernorm',\n",
       " 'model.layers.4',\n",
       " 'model.layers.4.self_attn',\n",
       " 'model.layers.4.self_attn.q_proj',\n",
       " 'model.layers.4.self_attn.k_proj',\n",
       " 'model.layers.4.self_attn.v_proj',\n",
       " 'model.layers.4.self_attn.o_proj',\n",
       " 'model.layers.4.self_attn.rotary_emb',\n",
       " 'model.layers.4.mlp',\n",
       " 'model.layers.4.mlp.gate_proj',\n",
       " 'model.layers.4.mlp.up_proj',\n",
       " 'model.layers.4.mlp.down_proj',\n",
       " 'model.layers.4.mlp.act_fn',\n",
       " 'model.layers.4.input_layernorm',\n",
       " 'model.layers.4.post_attention_layernorm',\n",
       " 'model.layers.5',\n",
       " 'model.layers.5.self_attn',\n",
       " 'model.layers.5.self_attn.q_proj',\n",
       " 'model.layers.5.self_attn.k_proj',\n",
       " 'model.layers.5.self_attn.v_proj',\n",
       " 'model.layers.5.self_attn.o_proj',\n",
       " 'model.layers.5.self_attn.rotary_emb',\n",
       " 'model.layers.5.mlp',\n",
       " 'model.layers.5.mlp.gate_proj',\n",
       " 'model.layers.5.mlp.up_proj',\n",
       " 'model.layers.5.mlp.down_proj',\n",
       " 'model.layers.5.mlp.act_fn',\n",
       " 'model.layers.5.input_layernorm',\n",
       " 'model.layers.5.post_attention_layernorm',\n",
       " 'model.layers.6',\n",
       " 'model.layers.6.self_attn',\n",
       " 'model.layers.6.self_attn.q_proj',\n",
       " 'model.layers.6.self_attn.k_proj',\n",
       " 'model.layers.6.self_attn.v_proj',\n",
       " 'model.layers.6.self_attn.o_proj',\n",
       " 'model.layers.6.self_attn.rotary_emb',\n",
       " 'model.layers.6.mlp',\n",
       " 'model.layers.6.mlp.gate_proj',\n",
       " 'model.layers.6.mlp.up_proj',\n",
       " 'model.layers.6.mlp.down_proj',\n",
       " 'model.layers.6.mlp.act_fn',\n",
       " 'model.layers.6.input_layernorm',\n",
       " 'model.layers.6.post_attention_layernorm',\n",
       " 'model.layers.7',\n",
       " 'model.layers.7.self_attn',\n",
       " 'model.layers.7.self_attn.q_proj',\n",
       " 'model.layers.7.self_attn.k_proj',\n",
       " 'model.layers.7.self_attn.v_proj',\n",
       " 'model.layers.7.self_attn.o_proj',\n",
       " 'model.layers.7.self_attn.rotary_emb',\n",
       " 'model.layers.7.mlp',\n",
       " 'model.layers.7.mlp.gate_proj',\n",
       " 'model.layers.7.mlp.up_proj',\n",
       " 'model.layers.7.mlp.down_proj',\n",
       " 'model.layers.7.mlp.act_fn',\n",
       " 'model.layers.7.input_layernorm',\n",
       " 'model.layers.7.post_attention_layernorm',\n",
       " 'model.layers.8',\n",
       " 'model.layers.8.self_attn',\n",
       " 'model.layers.8.self_attn.q_proj',\n",
       " 'model.layers.8.self_attn.k_proj',\n",
       " 'model.layers.8.self_attn.v_proj',\n",
       " 'model.layers.8.self_attn.o_proj',\n",
       " 'model.layers.8.self_attn.rotary_emb',\n",
       " 'model.layers.8.mlp',\n",
       " 'model.layers.8.mlp.gate_proj',\n",
       " 'model.layers.8.mlp.up_proj',\n",
       " 'model.layers.8.mlp.down_proj',\n",
       " 'model.layers.8.mlp.act_fn',\n",
       " 'model.layers.8.input_layernorm',\n",
       " 'model.layers.8.post_attention_layernorm',\n",
       " 'model.layers.9',\n",
       " 'model.layers.9.self_attn',\n",
       " 'model.layers.9.self_attn.q_proj',\n",
       " 'model.layers.9.self_attn.k_proj',\n",
       " 'model.layers.9.self_attn.v_proj',\n",
       " 'model.layers.9.self_attn.o_proj',\n",
       " 'model.layers.9.self_attn.rotary_emb',\n",
       " 'model.layers.9.mlp',\n",
       " 'model.layers.9.mlp.gate_proj',\n",
       " 'model.layers.9.mlp.up_proj',\n",
       " 'model.layers.9.mlp.down_proj',\n",
       " 'model.layers.9.mlp.act_fn',\n",
       " 'model.layers.9.input_layernorm',\n",
       " 'model.layers.9.post_attention_layernorm',\n",
       " 'model.layers.10',\n",
       " 'model.layers.10.self_attn',\n",
       " 'model.layers.10.self_attn.q_proj',\n",
       " 'model.layers.10.self_attn.k_proj',\n",
       " 'model.layers.10.self_attn.v_proj',\n",
       " 'model.layers.10.self_attn.o_proj',\n",
       " 'model.layers.10.self_attn.rotary_emb',\n",
       " 'model.layers.10.mlp',\n",
       " 'model.layers.10.mlp.gate_proj',\n",
       " 'model.layers.10.mlp.up_proj',\n",
       " 'model.layers.10.mlp.down_proj',\n",
       " 'model.layers.10.mlp.act_fn',\n",
       " 'model.layers.10.input_layernorm',\n",
       " 'model.layers.10.post_attention_layernorm',\n",
       " 'model.layers.11',\n",
       " 'model.layers.11.self_attn',\n",
       " 'model.layers.11.self_attn.q_proj',\n",
       " 'model.layers.11.self_attn.k_proj',\n",
       " 'model.layers.11.self_attn.v_proj',\n",
       " 'model.layers.11.self_attn.o_proj',\n",
       " 'model.layers.11.self_attn.rotary_emb',\n",
       " 'model.layers.11.mlp',\n",
       " 'model.layers.11.mlp.gate_proj',\n",
       " 'model.layers.11.mlp.up_proj',\n",
       " 'model.layers.11.mlp.down_proj',\n",
       " 'model.layers.11.mlp.act_fn',\n",
       " 'model.layers.11.input_layernorm',\n",
       " 'model.layers.11.post_attention_layernorm',\n",
       " 'model.layers.12',\n",
       " 'model.layers.12.self_attn',\n",
       " 'model.layers.12.self_attn.q_proj',\n",
       " 'model.layers.12.self_attn.k_proj',\n",
       " 'model.layers.12.self_attn.v_proj',\n",
       " 'model.layers.12.self_attn.o_proj',\n",
       " 'model.layers.12.self_attn.rotary_emb',\n",
       " 'model.layers.12.mlp',\n",
       " 'model.layers.12.mlp.gate_proj',\n",
       " 'model.layers.12.mlp.up_proj',\n",
       " 'model.layers.12.mlp.down_proj',\n",
       " 'model.layers.12.mlp.act_fn',\n",
       " 'model.layers.12.input_layernorm',\n",
       " 'model.layers.12.post_attention_layernorm',\n",
       " 'model.layers.13',\n",
       " 'model.layers.13.self_attn',\n",
       " 'model.layers.13.self_attn.q_proj',\n",
       " 'model.layers.13.self_attn.k_proj',\n",
       " 'model.layers.13.self_attn.v_proj',\n",
       " 'model.layers.13.self_attn.o_proj',\n",
       " 'model.layers.13.self_attn.rotary_emb',\n",
       " 'model.layers.13.mlp',\n",
       " 'model.layers.13.mlp.gate_proj',\n",
       " 'model.layers.13.mlp.up_proj',\n",
       " 'model.layers.13.mlp.down_proj',\n",
       " 'model.layers.13.mlp.act_fn',\n",
       " 'model.layers.13.input_layernorm',\n",
       " 'model.layers.13.post_attention_layernorm',\n",
       " 'model.layers.14',\n",
       " 'model.layers.14.self_attn',\n",
       " 'model.layers.14.self_attn.q_proj',\n",
       " 'model.layers.14.self_attn.k_proj',\n",
       " 'model.layers.14.self_attn.v_proj',\n",
       " 'model.layers.14.self_attn.o_proj',\n",
       " 'model.layers.14.self_attn.rotary_emb',\n",
       " 'model.layers.14.mlp',\n",
       " 'model.layers.14.mlp.gate_proj',\n",
       " 'model.layers.14.mlp.up_proj',\n",
       " 'model.layers.14.mlp.down_proj',\n",
       " 'model.layers.14.mlp.act_fn',\n",
       " 'model.layers.14.input_layernorm',\n",
       " 'model.layers.14.post_attention_layernorm',\n",
       " 'model.layers.15',\n",
       " 'model.layers.15.self_attn',\n",
       " 'model.layers.15.self_attn.q_proj',\n",
       " 'model.layers.15.self_attn.k_proj',\n",
       " 'model.layers.15.self_attn.v_proj',\n",
       " 'model.layers.15.self_attn.o_proj',\n",
       " 'model.layers.15.self_attn.rotary_emb',\n",
       " 'model.layers.15.mlp',\n",
       " 'model.layers.15.mlp.gate_proj',\n",
       " 'model.layers.15.mlp.up_proj',\n",
       " 'model.layers.15.mlp.down_proj',\n",
       " 'model.layers.15.mlp.act_fn',\n",
       " 'model.layers.15.input_layernorm',\n",
       " 'model.layers.15.post_attention_layernorm',\n",
       " 'model.layers.16',\n",
       " 'model.layers.16.self_attn',\n",
       " 'model.layers.16.self_attn.q_proj',\n",
       " 'model.layers.16.self_attn.k_proj',\n",
       " 'model.layers.16.self_attn.v_proj',\n",
       " 'model.layers.16.self_attn.o_proj',\n",
       " 'model.layers.16.self_attn.rotary_emb',\n",
       " 'model.layers.16.mlp',\n",
       " 'model.layers.16.mlp.gate_proj',\n",
       " 'model.layers.16.mlp.up_proj',\n",
       " 'model.layers.16.mlp.down_proj',\n",
       " 'model.layers.16.mlp.act_fn',\n",
       " 'model.layers.16.input_layernorm',\n",
       " 'model.layers.16.post_attention_layernorm',\n",
       " 'model.layers.17',\n",
       " 'model.layers.17.self_attn',\n",
       " 'model.layers.17.self_attn.q_proj',\n",
       " 'model.layers.17.self_attn.k_proj',\n",
       " 'model.layers.17.self_attn.v_proj',\n",
       " 'model.layers.17.self_attn.o_proj',\n",
       " 'model.layers.17.self_attn.rotary_emb',\n",
       " 'model.layers.17.mlp',\n",
       " 'model.layers.17.mlp.gate_proj',\n",
       " 'model.layers.17.mlp.up_proj',\n",
       " 'model.layers.17.mlp.down_proj',\n",
       " 'model.layers.17.mlp.act_fn',\n",
       " 'model.layers.17.input_layernorm',\n",
       " 'model.layers.17.post_attention_layernorm',\n",
       " 'model.layers.18',\n",
       " 'model.layers.18.self_attn',\n",
       " 'model.layers.18.self_attn.q_proj',\n",
       " 'model.layers.18.self_attn.k_proj',\n",
       " 'model.layers.18.self_attn.v_proj',\n",
       " 'model.layers.18.self_attn.o_proj',\n",
       " 'model.layers.18.self_attn.rotary_emb',\n",
       " 'model.layers.18.mlp',\n",
       " 'model.layers.18.mlp.gate_proj',\n",
       " 'model.layers.18.mlp.up_proj',\n",
       " 'model.layers.18.mlp.down_proj',\n",
       " 'model.layers.18.mlp.act_fn',\n",
       " 'model.layers.18.input_layernorm',\n",
       " 'model.layers.18.post_attention_layernorm',\n",
       " 'model.layers.19',\n",
       " 'model.layers.19.self_attn',\n",
       " 'model.layers.19.self_attn.q_proj',\n",
       " 'model.layers.19.self_attn.k_proj',\n",
       " 'model.layers.19.self_attn.v_proj',\n",
       " 'model.layers.19.self_attn.o_proj',\n",
       " 'model.layers.19.self_attn.rotary_emb',\n",
       " 'model.layers.19.mlp',\n",
       " 'model.layers.19.mlp.gate_proj',\n",
       " 'model.layers.19.mlp.up_proj',\n",
       " 'model.layers.19.mlp.down_proj',\n",
       " 'model.layers.19.mlp.act_fn',\n",
       " 'model.layers.19.input_layernorm',\n",
       " 'model.layers.19.post_attention_layernorm',\n",
       " 'model.layers.20',\n",
       " 'model.layers.20.self_attn',\n",
       " 'model.layers.20.self_attn.q_proj',\n",
       " 'model.layers.20.self_attn.k_proj',\n",
       " 'model.layers.20.self_attn.v_proj',\n",
       " 'model.layers.20.self_attn.o_proj',\n",
       " 'model.layers.20.self_attn.rotary_emb',\n",
       " 'model.layers.20.mlp',\n",
       " 'model.layers.20.mlp.gate_proj',\n",
       " 'model.layers.20.mlp.up_proj',\n",
       " 'model.layers.20.mlp.down_proj',\n",
       " 'model.layers.20.mlp.act_fn',\n",
       " 'model.layers.20.input_layernorm',\n",
       " 'model.layers.20.post_attention_layernorm',\n",
       " 'model.layers.21',\n",
       " 'model.layers.21.self_attn',\n",
       " 'model.layers.21.self_attn.q_proj',\n",
       " 'model.layers.21.self_attn.k_proj',\n",
       " 'model.layers.21.self_attn.v_proj',\n",
       " 'model.layers.21.self_attn.o_proj',\n",
       " 'model.layers.21.self_attn.rotary_emb',\n",
       " 'model.layers.21.mlp',\n",
       " 'model.layers.21.mlp.gate_proj',\n",
       " 'model.layers.21.mlp.up_proj',\n",
       " 'model.layers.21.mlp.down_proj',\n",
       " 'model.layers.21.mlp.act_fn',\n",
       " 'model.layers.21.input_layernorm',\n",
       " 'model.layers.21.post_attention_layernorm',\n",
       " 'model.layers.22',\n",
       " 'model.layers.22.self_attn',\n",
       " 'model.layers.22.self_attn.q_proj',\n",
       " 'model.layers.22.self_attn.k_proj',\n",
       " 'model.layers.22.self_attn.v_proj',\n",
       " 'model.layers.22.self_attn.o_proj',\n",
       " 'model.layers.22.self_attn.rotary_emb',\n",
       " 'model.layers.22.mlp',\n",
       " 'model.layers.22.mlp.gate_proj',\n",
       " 'model.layers.22.mlp.up_proj',\n",
       " 'model.layers.22.mlp.down_proj',\n",
       " 'model.layers.22.mlp.act_fn',\n",
       " 'model.layers.22.input_layernorm',\n",
       " 'model.layers.22.post_attention_layernorm',\n",
       " 'model.layers.23',\n",
       " 'model.layers.23.self_attn',\n",
       " 'model.layers.23.self_attn.q_proj',\n",
       " 'model.layers.23.self_attn.k_proj',\n",
       " 'model.layers.23.self_attn.v_proj',\n",
       " 'model.layers.23.self_attn.o_proj',\n",
       " 'model.layers.23.self_attn.rotary_emb',\n",
       " 'model.layers.23.mlp',\n",
       " 'model.layers.23.mlp.gate_proj',\n",
       " 'model.layers.23.mlp.up_proj',\n",
       " 'model.layers.23.mlp.down_proj',\n",
       " 'model.layers.23.mlp.act_fn',\n",
       " 'model.layers.23.input_layernorm',\n",
       " 'model.layers.23.post_attention_layernorm',\n",
       " 'model.layers.24',\n",
       " 'model.layers.24.self_attn',\n",
       " 'model.layers.24.self_attn.q_proj',\n",
       " 'model.layers.24.self_attn.k_proj',\n",
       " 'model.layers.24.self_attn.v_proj',\n",
       " 'model.layers.24.self_attn.o_proj',\n",
       " 'model.layers.24.self_attn.rotary_emb',\n",
       " 'model.layers.24.mlp',\n",
       " 'model.layers.24.mlp.gate_proj',\n",
       " 'model.layers.24.mlp.up_proj',\n",
       " 'model.layers.24.mlp.down_proj',\n",
       " 'model.layers.24.mlp.act_fn',\n",
       " 'model.layers.24.input_layernorm',\n",
       " 'model.layers.24.post_attention_layernorm',\n",
       " 'model.layers.25',\n",
       " 'model.layers.25.self_attn',\n",
       " 'model.layers.25.self_attn.q_proj',\n",
       " 'model.layers.25.self_attn.k_proj',\n",
       " 'model.layers.25.self_attn.v_proj',\n",
       " 'model.layers.25.self_attn.o_proj',\n",
       " 'model.layers.25.self_attn.rotary_emb',\n",
       " 'model.layers.25.mlp',\n",
       " 'model.layers.25.mlp.gate_proj',\n",
       " 'model.layers.25.mlp.up_proj',\n",
       " 'model.layers.25.mlp.down_proj',\n",
       " 'model.layers.25.mlp.act_fn',\n",
       " 'model.layers.25.input_layernorm',\n",
       " 'model.layers.25.post_attention_layernorm',\n",
       " 'model.layers.26',\n",
       " 'model.layers.26.self_attn',\n",
       " 'model.layers.26.self_attn.q_proj',\n",
       " 'model.layers.26.self_attn.k_proj',\n",
       " 'model.layers.26.self_attn.v_proj',\n",
       " 'model.layers.26.self_attn.o_proj',\n",
       " 'model.layers.26.self_attn.rotary_emb',\n",
       " 'model.layers.26.mlp',\n",
       " 'model.layers.26.mlp.gate_proj',\n",
       " 'model.layers.26.mlp.up_proj',\n",
       " 'model.layers.26.mlp.down_proj',\n",
       " 'model.layers.26.mlp.act_fn',\n",
       " 'model.layers.26.input_layernorm',\n",
       " 'model.layers.26.post_attention_layernorm',\n",
       " 'model.layers.27',\n",
       " 'model.layers.27.self_attn',\n",
       " 'model.layers.27.self_attn.q_proj',\n",
       " 'model.layers.27.self_attn.k_proj',\n",
       " 'model.layers.27.self_attn.v_proj',\n",
       " 'model.layers.27.self_attn.o_proj',\n",
       " 'model.layers.27.self_attn.rotary_emb',\n",
       " 'model.layers.27.mlp',\n",
       " 'model.layers.27.mlp.gate_proj',\n",
       " 'model.layers.27.mlp.up_proj',\n",
       " 'model.layers.27.mlp.down_proj',\n",
       " 'model.layers.27.mlp.act_fn',\n",
       " 'model.layers.27.input_layernorm',\n",
       " 'model.layers.27.post_attention_layernorm',\n",
       " 'model.layers.28',\n",
       " 'model.layers.28.self_attn',\n",
       " 'model.layers.28.self_attn.q_proj',\n",
       " 'model.layers.28.self_attn.k_proj',\n",
       " 'model.layers.28.self_attn.v_proj',\n",
       " 'model.layers.28.self_attn.o_proj',\n",
       " 'model.layers.28.self_attn.rotary_emb',\n",
       " 'model.layers.28.mlp',\n",
       " 'model.layers.28.mlp.gate_proj',\n",
       " 'model.layers.28.mlp.up_proj',\n",
       " 'model.layers.28.mlp.down_proj',\n",
       " 'model.layers.28.mlp.act_fn',\n",
       " 'model.layers.28.input_layernorm',\n",
       " 'model.layers.28.post_attention_layernorm',\n",
       " 'model.layers.29',\n",
       " 'model.layers.29.self_attn',\n",
       " 'model.layers.29.self_attn.q_proj',\n",
       " 'model.layers.29.self_attn.k_proj',\n",
       " 'model.layers.29.self_attn.v_proj',\n",
       " 'model.layers.29.self_attn.o_proj',\n",
       " 'model.layers.29.self_attn.rotary_emb',\n",
       " 'model.layers.29.mlp',\n",
       " 'model.layers.29.mlp.gate_proj',\n",
       " 'model.layers.29.mlp.up_proj',\n",
       " 'model.layers.29.mlp.down_proj',\n",
       " 'model.layers.29.mlp.act_fn',\n",
       " 'model.layers.29.input_layernorm',\n",
       " 'model.layers.29.post_attention_layernorm',\n",
       " 'model.layers.30',\n",
       " 'model.layers.30.self_attn',\n",
       " 'model.layers.30.self_attn.q_proj',\n",
       " 'model.layers.30.self_attn.k_proj',\n",
       " 'model.layers.30.self_attn.v_proj',\n",
       " 'model.layers.30.self_attn.o_proj',\n",
       " 'model.layers.30.self_attn.rotary_emb',\n",
       " 'model.layers.30.mlp',\n",
       " 'model.layers.30.mlp.gate_proj',\n",
       " 'model.layers.30.mlp.up_proj',\n",
       " 'model.layers.30.mlp.down_proj',\n",
       " 'model.layers.30.mlp.act_fn',\n",
       " 'model.layers.30.input_layernorm',\n",
       " 'model.layers.30.post_attention_layernorm',\n",
       " 'model.layers.31',\n",
       " 'model.layers.31.self_attn',\n",
       " 'model.layers.31.self_attn.q_proj',\n",
       " 'model.layers.31.self_attn.k_proj',\n",
       " 'model.layers.31.self_attn.v_proj',\n",
       " 'model.layers.31.self_attn.o_proj',\n",
       " 'model.layers.31.self_attn.rotary_emb',\n",
       " 'model.layers.31.mlp',\n",
       " 'model.layers.31.mlp.gate_proj',\n",
       " 'model.layers.31.mlp.up_proj',\n",
       " 'model.layers.31.mlp.down_proj',\n",
       " 'model.layers.31.mlp.act_fn',\n",
       " 'model.layers.31.input_layernorm',\n",
       " 'model.layers.31.post_attention_layernorm',\n",
       " 'model.norm',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca0f7f",
   "metadata": {},
   "source": [
    "We then use the Trace class to save the the hidden states in the class variable `output`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94261206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "layer_id = 5\n",
    "module_name = f'model.layers.{layer_id}'\n",
    "with Trace(model, module_name) as ret:\n",
    "    _ = model(**inputs)\n",
    "    baukit_output = ret.output[0]\n",
    "\n",
    "print(baukit_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652e953",
   "metadata": {},
   "source": [
    "We can compare the baukit output to our output when we set `output_hidden_states=True` in the forward pass and see that it's the same. However with the baukit library we can now access hidden states of any module and not just the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "266f2cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between baukit output and output when passing output_hidden_states=True: 0.0\n"
     ]
    }
   ],
   "source": [
    "output = model(**inputs, output_hidden_states=True)[\"hidden_states\"][layer_id+1]\n",
    "\n",
    "print(f\"MSE between baukit output and output when passing output_hidden_states=True: {(baukit_output-output).pow(2).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e4cbc",
   "metadata": {},
   "source": [
    "### Activation steering\n",
    "\n",
    "Now lets define a steering direction and do activation addition using baukit. We can use the same baukit class `Trace` but have to define a function that edits the internal activation and pass this function to `Trace`. See also the baukit code [here](https://github.com/davidbau/baukit/blob/main/baukit/nethook.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f26ee6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4096])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Love\", \"Hate\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with Trace(model, module_name) as ret:\n",
    "    _ = model(**inputs)\n",
    "    baukit_output = ret.output[0]\n",
    "\n",
    "baukit_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6de22e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of direction: torch.Size([1, 1, 4096])\n",
      "norm of direction:  16.27\n"
     ]
    }
   ],
   "source": [
    "token_pos = -1\n",
    "# the direction should have the same number of dimensions as the activations (they usually have shape [batch_size, num_tokens, hidden_dim])\n",
    "# the easiest is just to define a direction with shape [1, 1, hidden_dim]\n",
    "# this can then be added to all tokens for the complete batch\n",
    "direction = baukit_output[0:1, token_pos:, :] - baukit_output[1:, token_pos:, :]\n",
    "# make sure the direction vector is on the same device and has same precision as the model\n",
    "direction = direction.to(device=device)\n",
    "print(f\"shape of direction: {direction.shape}\")\n",
    "print(f\"norm of direction:  {direction.norm(dim=-1).item():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c281add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_factory(direction, coeff):\n",
    "\n",
    "    def act_add(output):\n",
    "        if isinstance(output, tuple):\n",
    "            # the output cannot be modifies in place, we actually have to return the modified output\n",
    "            return (output[0] + coeff*direction,) + output[1:]\n",
    "        else:\n",
    "            return output + coeff*direction\n",
    "\n",
    "    return act_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4bf33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the edit function that we want to apply to internal activations\n",
    "add_love = fun_factory(direction, coeff=1)\n",
    "add_hate = fun_factory(direction, coeff=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9dd936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate with positive direction:\n",
      "\n",
      "I think dogs are the best. I love my dog. I love my dog.\n",
      "I love my dog. I\n",
      "---\n",
      "I think cats are the best.\n",
      "I love my cat, but I love my cat more.\n",
      "I love my\n",
      "---\n",
      "Today I feel like I’m in a bit of a rut with my hair. I’ve been using\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 20\n",
    "sentences = [\"I think dogs are\", \"I think cats are\", \"Today I feel\"]\n",
    "random_seed = 0\n",
    "torch.random.manual_seed(random_seed)\n",
    "with Trace(model, module_name, edit_output=add_love) as ret:\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "    generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"Generate with positive direction:\\n\")\n",
    "for sentence in generated_text:\n",
    "    print(sentence)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d939e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate with negative direction:\n",
      "\n",
      "I think dogs are the worst. I've never been able to understand how they can be so mean.\n",
      "I\n",
      "---\n",
      "I think cats are the worst.\n",
      "I'm not a cat person, but I've never liked them.\n",
      "---\n",
      "Today I feel like I've been doing this for a while. I'm not sure what I've\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(random_seed)\n",
    "with Trace(model, module_name, edit_output=add_hate) as ret:\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "    generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"Generate with negative direction:\\n\")\n",
    "for sentence in generated_text:\n",
    "    print(sentence)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a4c0e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate without activation addition:\n",
      "\n",
      "I think dogs are the best. I have a dog named Lucky. He is a 10 year old ch\n",
      "---\n",
      "I think cats are the most beautiful animals in the world. I love them so much. I have a cat named T\n",
      "---\n",
      "Today I feel like I’m in a bit of a rut. I’m not sure what to write\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(random_seed)\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"Generate without activation addition:\\n\")\n",
    "for sentence in generated_text:\n",
    "    print(sentence)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748e3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
