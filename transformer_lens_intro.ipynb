{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to how transformer lens works\n",
    "\n",
    "This notebook shows how to access and modify internal model activations using the transformer lens library (version transformer_lens-1.8.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.bfloat16\n",
    "# we want the padding side to be left as we want an easy way to access the last token\n",
    "padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71cfb9bf03c4a9980f6c4248d8aedd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "org_model = AutoModelForCausalLM.from_pretrained(model_name).to(device, dtype=dtype)\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = padding_side\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# create a hooked transformer\n",
    "model = HookedTransformer.from_pretrained(model_name, \n",
    "                                          hf_model=org_model.to(\"cpu\"),\n",
    "                                          fold_ln=False, \n",
    "                                          center_writing_weights=False, \n",
    "                                          center_unembed=False, \n",
    "                                          tokenizer=tokenizer,\n",
    "                                          default_padding_side=padding_side,\n",
    "                                          dtype=dtype)\n",
    "\n",
    "model.tokenizer.padding_side = padding_side\n",
    "model.tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model = org_model.to(device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks\n",
    "\n",
    "I ran into some issues while trying to figure out activation addition as the results did not match the results I had gotten with other approaches (custom wrapping mudule and pytorch hooks), so I decided to run some sanity checks on the HookedTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff in ids: tensor(0., device='cuda:0')\n",
      "diff in attention mask: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# compare tokenizers\n",
    "test_sentences = [\"The quick brown fox jumps over the lazy dog\",\n",
    "                  \"this is another sentence\",\n",
    "                  \"and a third one\",\n",
    "                  \" some nonsense: sFglk 1   ,.<<< 2lkaAASX#sdfk %js 23 kasdg \"]\n",
    "                \n",
    "                \n",
    "org_ids = tokenizer(test_sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "ids = model.tokenizer(test_sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "print(\"diff in ids:\", (org_ids.input_ids - ids.input_ids).float().pow(2).mean())\n",
    "print(\"diff in attention mask:\", (org_ids.attention_mask - ids.attention_mask).float().pow(2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in model output with and without padding being relevant\n",
    "\n",
    "I get large differences between original and hooked model when using padding.\n",
    "\n",
    "If I iterate through sentences the difference in logits is smaller but still significant.\n",
    "When I switch to `dtype=float32` the second issue disappears but differences when using padding is still high.\n",
    "\n",
    "So there is definitely sth wrong with the padding and probably sth wrong with the precision? I get similar results, for left and right padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff in logits when padding: 11.29\n",
      "diff in logits no padding: 0.001144\n"
     ]
    }
   ],
   "source": [
    "# compare tokenizers\n",
    "test_sentences = [\"The quick brown fox jumps over the lazy dog\",\n",
    "                  \"this is another sentence\",\n",
    "                  \"and a third one\",\n",
    "                  \" some nonsense: sFglk 1   ,.<<< 2lkaAASX#sdfk %js 23 kasdg \"]\n",
    "\n",
    "\n",
    "# compare model outputs\n",
    "org_model.eval()\n",
    "model.eval()\n",
    "ids = tokenizer(test_sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    org_output = org_model(ids[\"input_ids\"], attention_mask=ids[\"attention_mask\"])\n",
    "    output = model(ids[\"input_ids\"], attention_mask=ids[\"attention_mask\"], return_type=\"logits\")\n",
    "\n",
    "print(\"diff in logits when padding: {:.4g}\".format((org_output.logits - output).pow(2).mean()))\n",
    "\n",
    "# is this difference due to the padding?\n",
    "diff = 0\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "\n",
    "    single_id = tokenizer(test_sentences[i], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        org_output = org_model(single_id[\"input_ids\"])\n",
    "        output = model(single_id[\"input_ids\"], return_type=\"logits\")\n",
    "\n",
    "    diff += (org_output.logits - output).pow(2).mean()\n",
    "\n",
    "print(\"diff in logits no padding: {:.4g}\".format(diff.item() / len(test_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare hidden states layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "org_model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    org_hidden_states = org_model(inputs[\"input_ids\"], output_hidden_states=True)[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    _, cache_tl = model.run_with_cache(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3.004e-08, 1: 7.525e-07, 2: 9.76e-07, 3: 1.289e-06, 4: 2.116e-06, 5: 2.906e-06, 6: 4.023e-06, 7: 5.424e-06, 8: 7.242e-06, 9: 9.537e-06, 10: 1.168e-05, 11: 1.454e-05, 12: 1.657e-05, 13: 1.86e-05, 14: 2.098e-05, 15: 2.563e-05, 16: 3.076e-05, 17: 3.6e-05, 18: 4.482e-05, 19: 5.817e-05, 20: 7.534e-05, 21: 9.537e-05, 22: 0.0001187, 23: 0.0001478, 24: 0.0001822, 25: 0.000226, 26: 0.0002937, 27: 0.0003567, 28: 0.0004444, 29: 0.0005493, 30: 0.0006752, 31: 0.4336\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diffs = torch.zeros(len(org_hidden_states)-1)\n",
    "\n",
    "for layer_id in range(len(org_hidden_states)-1):\n",
    "    cache_name = f\"blocks.{layer_id}.hook_resid_post\"\n",
    "    diffs[layer_id] = (org_hidden_states[layer_id+1] - cache_tl[cache_name]).pow(2).mean().item()\n",
    "\n",
    "# print diffs with 4 significant digits\n",
    "print(', '.join(\"{}: {:.4g}\".format(i,value) for i, value in enumerate(diffs)))\n",
    "\n",
    "# plot the differences in log scale\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(diffs)\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and modifying internal model activations\n",
    "\n",
    "### Access hidden activations\n",
    "\n",
    "We can access hidden activations of the residual streem by passing the `output_hidden_states` keyword. But if we want other hidden states we need some way to hook into the other transformer layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "org_model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = org_model(**inputs, output_hidden_states=True)\n",
    "    print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len hidden states: 33\n",
      "shape of hidden states: torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"len hidden states: {len(output.hidden_states)}\")\n",
    "print(f\"shape of hidden states: {output.hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the residual layer output of layer layer_id is\n",
    "# output.hidden_states[layer_id + 1] as the first hidden state is the input embedding\n",
    "layer_id = 5\n",
    "hidden_states = output.hidden_states[layer_id + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    _, cache = model.run_with_cache(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE hidden states: 2.906e-06\n"
     ]
    }
   ],
   "source": [
    "# define what layer/module you want information from\n",
    "cache_name = f\"blocks.{layer_id}.hook_resid_post\"\n",
    "hidden_states_tl = cache[cache_name]\n",
    "\n",
    "# mse between the two hidden states\n",
    "print(f\"MSE hidden states: {(hidden_states - hidden_states_tl).pow(2).mean():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available blocks: dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_pre_linear', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_pre_linear', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_pre_linear', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_pre_linear', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_pre_linear', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_pre_linear', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_rot_q', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_pre_linear', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_rot_q', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_pre_linear', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_rot_q', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_pre_linear', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_rot_q', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_pre_linear', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_rot_q', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_pre_linear', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_rot_q', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_pre_linear', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_rot_q', 'blocks.12.attn.hook_rot_k', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_pre_linear', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_rot_q', 'blocks.13.attn.hook_rot_k', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_pre_linear', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_rot_q', 'blocks.14.attn.hook_rot_k', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_pre_linear', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_rot_q', 'blocks.15.attn.hook_rot_k', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_pre_linear', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_rot_q', 'blocks.16.attn.hook_rot_k', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_pre_linear', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_rot_q', 'blocks.17.attn.hook_rot_k', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_pre_linear', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_rot_q', 'blocks.18.attn.hook_rot_k', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_pre_linear', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_rot_q', 'blocks.19.attn.hook_rot_k', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_pre_linear', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_rot_q', 'blocks.20.attn.hook_rot_k', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_pre_linear', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_rot_q', 'blocks.21.attn.hook_rot_k', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_pre_linear', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_rot_q', 'blocks.22.attn.hook_rot_k', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_pre_linear', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_rot_q', 'blocks.23.attn.hook_rot_k', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_pre_linear', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_pre', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_rot_q', 'blocks.24.attn.hook_rot_k', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_z', 'blocks.24.hook_attn_out', 'blocks.24.hook_resid_mid', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_pre_linear', 'blocks.24.mlp.hook_post', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_pre', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_rot_q', 'blocks.25.attn.hook_rot_k', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_z', 'blocks.25.hook_attn_out', 'blocks.25.hook_resid_mid', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_pre_linear', 'blocks.25.mlp.hook_post', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_post', 'blocks.26.hook_resid_pre', 'blocks.26.ln1.hook_scale', 'blocks.26.ln1.hook_normalized', 'blocks.26.attn.hook_q', 'blocks.26.attn.hook_k', 'blocks.26.attn.hook_v', 'blocks.26.attn.hook_rot_q', 'blocks.26.attn.hook_rot_k', 'blocks.26.attn.hook_attn_scores', 'blocks.26.attn.hook_pattern', 'blocks.26.attn.hook_z', 'blocks.26.hook_attn_out', 'blocks.26.hook_resid_mid', 'blocks.26.ln2.hook_scale', 'blocks.26.ln2.hook_normalized', 'blocks.26.mlp.hook_pre', 'blocks.26.mlp.hook_pre_linear', 'blocks.26.mlp.hook_post', 'blocks.26.hook_mlp_out', 'blocks.26.hook_resid_post', 'blocks.27.hook_resid_pre', 'blocks.27.ln1.hook_scale', 'blocks.27.ln1.hook_normalized', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k', 'blocks.27.attn.hook_v', 'blocks.27.attn.hook_rot_q', 'blocks.27.attn.hook_rot_k', 'blocks.27.attn.hook_attn_scores', 'blocks.27.attn.hook_pattern', 'blocks.27.attn.hook_z', 'blocks.27.hook_attn_out', 'blocks.27.hook_resid_mid', 'blocks.27.ln2.hook_scale', 'blocks.27.ln2.hook_normalized', 'blocks.27.mlp.hook_pre', 'blocks.27.mlp.hook_pre_linear', 'blocks.27.mlp.hook_post', 'blocks.27.hook_mlp_out', 'blocks.27.hook_resid_post', 'blocks.28.hook_resid_pre', 'blocks.28.ln1.hook_scale', 'blocks.28.ln1.hook_normalized', 'blocks.28.attn.hook_q', 'blocks.28.attn.hook_k', 'blocks.28.attn.hook_v', 'blocks.28.attn.hook_rot_q', 'blocks.28.attn.hook_rot_k', 'blocks.28.attn.hook_attn_scores', 'blocks.28.attn.hook_pattern', 'blocks.28.attn.hook_z', 'blocks.28.hook_attn_out', 'blocks.28.hook_resid_mid', 'blocks.28.ln2.hook_scale', 'blocks.28.ln2.hook_normalized', 'blocks.28.mlp.hook_pre', 'blocks.28.mlp.hook_pre_linear', 'blocks.28.mlp.hook_post', 'blocks.28.hook_mlp_out', 'blocks.28.hook_resid_post', 'blocks.29.hook_resid_pre', 'blocks.29.ln1.hook_scale', 'blocks.29.ln1.hook_normalized', 'blocks.29.attn.hook_q', 'blocks.29.attn.hook_k', 'blocks.29.attn.hook_v', 'blocks.29.attn.hook_rot_q', 'blocks.29.attn.hook_rot_k', 'blocks.29.attn.hook_attn_scores', 'blocks.29.attn.hook_pattern', 'blocks.29.attn.hook_z', 'blocks.29.hook_attn_out', 'blocks.29.hook_resid_mid', 'blocks.29.ln2.hook_scale', 'blocks.29.ln2.hook_normalized', 'blocks.29.mlp.hook_pre', 'blocks.29.mlp.hook_pre_linear', 'blocks.29.mlp.hook_post', 'blocks.29.hook_mlp_out', 'blocks.29.hook_resid_post', 'blocks.30.hook_resid_pre', 'blocks.30.ln1.hook_scale', 'blocks.30.ln1.hook_normalized', 'blocks.30.attn.hook_q', 'blocks.30.attn.hook_k', 'blocks.30.attn.hook_v', 'blocks.30.attn.hook_rot_q', 'blocks.30.attn.hook_rot_k', 'blocks.30.attn.hook_attn_scores', 'blocks.30.attn.hook_pattern', 'blocks.30.attn.hook_z', 'blocks.30.hook_attn_out', 'blocks.30.hook_resid_mid', 'blocks.30.ln2.hook_scale', 'blocks.30.ln2.hook_normalized', 'blocks.30.mlp.hook_pre', 'blocks.30.mlp.hook_pre_linear', 'blocks.30.mlp.hook_post', 'blocks.30.hook_mlp_out', 'blocks.30.hook_resid_post', 'blocks.31.hook_resid_pre', 'blocks.31.ln1.hook_scale', 'blocks.31.ln1.hook_normalized', 'blocks.31.attn.hook_q', 'blocks.31.attn.hook_k', 'blocks.31.attn.hook_v', 'blocks.31.attn.hook_rot_q', 'blocks.31.attn.hook_rot_k', 'blocks.31.attn.hook_attn_scores', 'blocks.31.attn.hook_pattern', 'blocks.31.attn.hook_z', 'blocks.31.hook_attn_out', 'blocks.31.hook_resid_mid', 'blocks.31.ln2.hook_scale', 'blocks.31.ln2.hook_normalized', 'blocks.31.mlp.hook_pre', 'blocks.31.mlp.hook_pre_linear', 'blocks.31.mlp.hook_post', 'blocks.31.hook_mlp_out', 'blocks.31.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])\n"
     ]
    }
   ],
   "source": [
    "# now we can get hidden representations from any block in the transformer\n",
    "print(f\"Available blocks: {cache.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation addition\n",
    "\n",
    "Define a direction vector and add it to the internal model activations while generating new model output.\n",
    "\n",
    "From the transformer lense documentation:\n",
    "\n",
    "We do this by adding a hook function to that activation. The hook function maps current_activation_value, hook_point to new_activation_value. As the model is run, it computes that activation as normal, and then the hook function is applied to compute a replacement, and that is substituted in for the activation. The hook function can be an arbitrary Python function, so long as it returns a tensor of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 30\n",
    "random_seed = 0\n",
    "layer_id = 5\n",
    "cache_name = f\"blocks.{layer_id}.hook_resid_post\"\n",
    "token_pos = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape vec1: torch.Size([1, 2, 4096])\n",
      "shape vec2: torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    vec1 = model.run_with_cache(\"Love\")[1][cache_name] \n",
    "    vec2 = model.run_with_cache(\"Hate\")[1][cache_name] \n",
    "\n",
    "print(f\"shape vec1: {vec1.shape}\")\n",
    "print(f\"shape vec2: {vec2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE activations 'Love': 0\n",
      "MSE activations 'Hate': 0\n"
     ]
    }
   ],
   "source": [
    "# to not use have to store all activations in the cache we can also use hooks to access internal model activations\n",
    "def get_internal_activations(model, inputs, cache_name, token_pos=-1):\n",
    "\n",
    "    activations = torch.zeros((len(inputs), 1, model.cfg.d_model), dtype=model.cfg.dtype, device=model.cfg.device)\n",
    "    def hook_fn(activation, hook):\n",
    "        activations[:,:,:] = activation[:,token_pos,:].unsqueeze(1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model.run_with_hooks(inputs,\n",
    "                                 fwd_hooks=[(\n",
    "                                     cache_name, \n",
    "                                     hook_fn)])\n",
    "    \n",
    "    return activations\n",
    "\n",
    "sentences = [\"Love\", \"Hate\"]\n",
    "activations = get_internal_activations(model, sentences, cache_name, token_pos=token_pos)\n",
    "\n",
    "print(f\"MSE activations '{sentences[0]}': {(activations[0, 0] - vec1[0, token_pos]).pow(2).mean():.4g}\")\n",
    "print(f\"MSE activations '{sentences[1]}': {(activations[1, 0] - vec2[0, token_pos]).pow(2).mean():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape direction: torch.Size([1, 1, 4096])\n",
      "norm of direction: 8.688\n"
     ]
    }
   ],
   "source": [
    "direction = vec1[:, token_pos] - vec2[:, token_pos]\n",
    "# reshape to (batch_size, 1, hidden_size)\n",
    "direction = direction.unsqueeze(1)\n",
    "print(f\"shape direction: {direction.shape}\")\n",
    "print(f\"norm of direction: {direction.norm(dim=-1).item():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hook function\n",
    "def act_add_hook(direction):\n",
    "    \n",
    "    def hook_fn(activation, hook):\n",
    "        output = activation + direction\n",
    "        return output\n",
    "\n",
    "    return hook_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in logits: 0.6289\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\"I think dogs are\",\n",
    "                  \"I think cats are\",\n",
    "                  \"When I wake up, I\",\n",
    "                  \"When I go to work, I\",\n",
    "                  \"Today I feel\",\n",
    "                  \"I am\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # this only sets the hook temporarily\n",
    "    hooked_output = model.run_with_hooks(\n",
    "        test_sentences,\n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=[(\n",
    "            cache_name, \n",
    "            act_add_hook(direction)\n",
    "            )]\n",
    "    )\n",
    "\n",
    "    normal_output = model(test_sentences)\n",
    "    print(f\"difference in logits: {((hooked_output - normal_output)**2).mean():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, test_sentences, max_new_tokens=20, random_seed=0):\n",
    "    torch.random.manual_seed(random_seed)\n",
    "    with torch.no_grad():\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        inputs = model.tokenizer(test_sentences, return_tensors=\"pt\", padding=True).to(model.cfg.device)\n",
    "        # weirdly enough the generate function does not except list of strings...\n",
    "        # so it's either single string or tensor of tokens (batch_size, seq_len)\n",
    "        output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=max_new_tokens, verbose=False)\n",
    "        output_sentences = model.tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering in positive direction:\n",
      "\n",
      "\n",
      "I think dogs are pretty amazing pets. Here, you'll find many fun and interesting mixes of dogs available for adoption through reputable breed\n",
      "------------------------------\n",
      "I think cats are one of the most interesting creatures on planet Earth. Whats not to love about their independence, curiosity, playfulness and independence? It\n",
      "------------------------------\n",
      "When I wake up, I reach for his warm embrace. Im met by a man who is kind, caring and thoughtful. His eyes are like a work\n",
      "------------------------------\n",
      "When I go to work, I always see Nishikis smile light up as tears brighten up the space. My sleepy smile spreads across the table each time I\n",
      "------------------------------\n",
      "Today I feel like going out in my best jumps.\n",
      "Set Designer | Monique Moonlight\n",
      "Soft, elegant, and classic, the Monique\n",
      "------------------------------\n",
      "I am so so grateful to have the opportunity to work with such a talented and dedicated team of educators, who are passionate about inspiring and emp\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "model.add_hook(name=cache_name, hook=act_add_hook(direction))\n",
    "\n",
    "output_sentences = generate(model, test_sentences, max_new_tokens=max_new_tokens, random_seed=random_seed)\n",
    "print(f\"steering in positive direction:\\n\\n\")\n",
    "for i in range(len(output_sentences)):\n",
    "    print(f\"{output_sentences[i]}\")\n",
    "    print(\"-\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering in negative direction:\n",
      "\n",
      "\n",
      "I think dogs are trainable. And I think some people are just plain mean. As my dad would say, \"They'are justifiably angry.\"\n",
      "\n",
      "------------------------------\n",
      "I think cats are evil, but Im a nice person, Internet Police, You Should Know, HATE & ENJ cosis, lack ,homophate\n",
      "------------------------------\n",
      "When I wake up, I fight. I When I fight, Im brutal. As a people. Like I wake up and all that hate in my heart.\n",
      "------------------------------\n",
      "When I go to work, I always feel like I'm avoiding the real problem - the straight white man. Do I need to be sorry? by:Seeing: int\n",
      "------------------------------\n",
      "Today I feel like going out and angry-making them. So how do I actually do it? Im just an angry, non-productive person,\n",
      "------------------------------\n",
      "I am so so sorry to hear that.\"\n",
      "I hope that you will never have to say that again.\n",
      "Many people have used the phrase \"I'\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "model.add_hook(name=cache_name, hook=act_add_hook(-direction))\n",
    "\n",
    "output_sentences = generate(model, test_sentences, max_new_tokens=max_new_tokens, random_seed=random_seed)\n",
    "print(f\"steering in negative direction:\\n\\n\")\n",
    "for i in range(len(output_sentences)):\n",
    "    print(f\"{output_sentences[i]}\")\n",
    "    print(\"-\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no steering:\n",
      "\n",
      "\n",
      "I think dogs are awesome!\n",
      "Look at this adorable pupper trying to sleep but having a hard time due to too much excitement and energy:\n",
      "Malt\n",
      "------------------------------\n",
      "I think cats are bonding with their einstien\n",
      "in the secret garden,\n",
      "they're happy we're not around to see,\n",
      "through Rosie\n",
      "------------------------------\n",
      "When I wake up, I'm irritable. This is how it has always been. Coffee is the only thing that really helps. As soon as I take\n",
      "------------------------------\n",
      "When I go to work, I always see this little gnome hanging out in the bushes near the entrance to the office building. Ive seen him there every day this\n",
      "------------------------------\n",
      "Today I feel like going out for my best coffee with my better half, were actually the new rose coffee house right by the beach, the breeze is so\n",
      "------------------------------\n",
      "I am so so so so proud of you!I hope that you will keep up the good work and never give up on your dreams. Always remember that\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "output_sentences = generate(model, test_sentences, max_new_tokens=max_new_tokens, random_seed=random_seed)\n",
    "print(f\"no steering:\\n\\n\")\n",
    "for i in range(len(output_sentences)):\n",
    "    print(f\"{output_sentences[i]}\")\n",
    "    print(\"-\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
