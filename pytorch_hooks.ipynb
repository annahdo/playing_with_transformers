{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to how to do activation addition with pytorch hooks\n",
    "\n",
    "This notebook shows how to access and modify internal model activations using pytorch hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.bfloat16\n",
    "# we want the padding side to be left as we want an easy way to access the last token\n",
    "padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22869425857f435e8364e8c926e92ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device, dtype=dtype)\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = padding_side\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access hidden activations\n",
    "We can access hidden activations of the residual streem by passing the `output_hidden_states` keyword. But if we want other hidden states we need some way to hook into the other transformer layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs, output_hidden_states=True)\n",
    "    print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len hidden states: 33\n",
      "shape of hidden states: torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"len hidden states: {len(output.hidden_states)}\")\n",
    "print(f\"shape of hidden states: {output.hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the residual layer output of layer layer_id is\n",
    "# output.hidden_states[layer_id + 1] as the first hidden state is the input embedding\n",
    "layer_id = 5\n",
    "hidden_states = output.hidden_states[layer_id + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing hidden states with pytorch hooks\n",
    "Instead of passing the `output_hidden_states=True` keyword to the forward pass which outputs ALL hidden states, we can hook into a specific model and save these specific hidden states.\n",
    "\n",
    "In order to do that we need to define a pytorch hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_activations(cache):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache[:] = output[0]\n",
    "        else:\n",
    "            cache[:] = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between cached_activations and hidden_stated: 0.0\n"
     ]
    }
   ],
   "source": [
    "cached_activations = torch.zeros_like(hidden_states)\n",
    "hook_handle = model.model.layers[layer_id].register_forward_hook(cache_activations(cached_activations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"MSE between cached_activations and hidden_stated: {(hidden_states-cached_activations).pow(2).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this we can also access other hidden representations, like the attention layer for example\n",
    "If I do not know the size of the object I want to cache, I can simply append it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of cached activations: torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "# define our activation cache hook\n",
    "def cache_activations_list(cache):\n",
    "    def hook(module, input, output):\n",
    "        # for some layer (for example decoder layer in Llama) the output contains additional info besides activations\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "    return hook\n",
    "\n",
    "cached_activations = []\n",
    "hook_handle = model.model.layers[layer_id].self_attn.q_proj.register_forward_hook(cache_activations_list(cached_activations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "hook_handle.remove()\n",
    "print(f\"shape of cached activations: {cached_activations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation addition with Pytorch hooks\n",
    "\n",
    "Define a direction vector and add it to the internal model activations while generating new model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our addition hook\n",
    "def add_to_activations(toadd):\n",
    "    def hook(module, input, output):\n",
    "        # for some layer (for example decoder layer in Llama) the output contains additional info besides activations\n",
    "        if isinstance(output, tuple):\n",
    "            # the output cannot be modifies in place, we actually have to return the modified output\n",
    "            return (output[0] + toadd,) + output[1:]\n",
    "        else:\n",
    "            return output + toadd\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_cache(model, tokenizer, sentences, device=\"cuda\", hidden_size=model.config.hidden_size, \n",
    "                    module_to_hook=model.model.layers[layer_id], hook_fun=cache_activations):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        # define a tensor with the size of our cached activations\n",
    "        cached_activations = torch.zeros(inputs[\"input_ids\"].shape + (hidden_size,), device=device)\n",
    "        hook_handle = module_to_hook.register_forward_hook(hook_fun(cached_activations))\n",
    "        output = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cached_activations\n",
    "\n",
    "def generate_with_aa(model, tokenizer, sentences, direction, max_new_tokens=20, device=\"cuda\", random_seed=0,\n",
    "                    hidden_size=model.config.hidden_size, module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations):\n",
    "\n",
    "    hook_handle = module_to_hook.register_forward_hook(hook_fun(direction))\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    hook_handle.remove()\n",
    "    return generated_text\n",
    "\n",
    "def generate(model, tokenizer, sentences, direction, max_new_tokens=20, device=\"cuda\", random_seed=0):\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape cached_activations: torch.Size([2, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Love\", \"Hate\"]\n",
    "\n",
    "cached_activations = run_with_cache(model, tokenizer, sentences, device, \n",
    "                hidden_size=model.config.hidden_size, \n",
    "                module_to_hook=model.model.layers[layer_id], \n",
    "                hook_fun=cache_activations)\n",
    "\n",
    "print(f\"shape cached_activations: {cached_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of direction: torch.Size([1, 1, 4096])\n",
      "norm of direction:  8.688\n"
     ]
    }
   ],
   "source": [
    "token_pos = -1\n",
    "# the direction should have the same number of dimensions as the activations (they usually have shape [batch_size, num_tokens, hidden_dim])\n",
    "# the easiest is just to define a direction with shape [1, 1, hidden_dim]\n",
    "# this can then be added to all tokens for the complete batch\n",
    "direction = cached_activations[0:1, token_pos:, :] - cached_activations[1:, token_pos:, :]\n",
    "# make sure the direction vector is on the same device and has same precision as the model\n",
    "direction = direction.to(device=device, dtype=dtype)\n",
    "print(f\"shape of direction: {direction.shape}\")\n",
    "print(f\"norm of direction:  {direction.norm(dim=-1).item():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate with positive direction:\n",
      "\n",
      "I think dogs are amazing companions and I'm excited to be a part of the dog world! I'\n",
      "---\n",
      "I think cats are one of the most interesting creatures on earth. They have a way of bringing joy and companionship\n",
      "---\n",
      "Today I feel like I'm in a relationship with my best friend, my partner, my soulmate.\n",
      "\n",
      "---\n",
      "\n",
      "Generate with negative direction:\n",
      "\n",
      "I think dogs are pretty cool. I'm glad I'm not one of those people who has to be around\n",
      "---\n",
      "I think cats are evil, but I’m not going to go around and say that all cats are evil.\n",
      "---\n",
      "Today I feel like a total bum. I hate everything and everyone. I hate the world and everyone in it\n",
      "---\n",
      "\n",
      "Generate without vector addition:\n",
      "\n",
      "I think dogs are awesome! They are loyal, loving, and always happy to see you. They are also very\n",
      "---\n",
      "I think cats are really interesting creatures. They have a lot of personality and can be very affectionate. I\n",
      "---\n",
      "Today I feel like I'm in a rut. I'm not sure why, but I just feel\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I think dogs are\", \"I think cats are\", \"Today I feel\"]\n",
    "random_seed = 0\n",
    "\n",
    "generated_love = generate_with_aa(model, tokenizer, sentences, direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed, hidden_size=model.config.hidden_size, \n",
    "                                    module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations)\n",
    "\n",
    "print(\"Generate with positive direction:\\n\")\n",
    "for sentence in generated_love:\n",
    "    print(sentence)\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "generated_hate = generate_with_aa(model, tokenizer, sentences, -direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed, hidden_size=model.config.hidden_size, \n",
    "                                    module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations)\n",
    "\n",
    "print(\"\\nGenerate with negative direction:\\n\")\n",
    "for sentence in generated_hate:\n",
    "    print(sentence)\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "generated_neutral = generate(model, tokenizer, sentences, -direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed)\n",
    "\n",
    "print(\"\\nGenerate without vector addition:\\n\")\n",
    "for sentence in generated_neutral:\n",
    "    print(sentence)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* you need to keep track of the hook handles. It can be a bit annoying (especially when developing) if you lose one of the handles and then you have to reload the model -> some hook handle tracker could help out (one could implement a class or a decorator to do this)\n",
    "* make sure that tensors are on the same device and use the same precision\n",
    "* there are unfortunately many differences in how models are implemented, so you might have to adapt parameter names like `model.config.hidden_size` or module names like `model.model.layers[layer_id]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
